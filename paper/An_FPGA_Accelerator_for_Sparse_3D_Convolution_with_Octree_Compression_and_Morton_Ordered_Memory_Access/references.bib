@inproceedings{rl2021,
author = {Li, Ruihao and Liu, Ke and Cai, Xiaojun and Zhao, Mengying and John, Lizy K. and Jia, Zhiping},
title = {Improving CNN performance on FPGA clusters through topology exploration},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441893},
doi = {10.1145/3412841.3441893},
abstract = {Field Programmable Gate Array (FPGA) platform has been a popular choice for deploying Convolution Neural Networks (CNNs) as a result of its high parallelism and low energy consumption. Due to the limited on-chip computation and storage resources, FPGA clusters are becoming promising candidates to improve CNN throughputs. In this paper, we first put forward strategies to optimize the inter-board resource allocation in FPGA clusters. Then we model the multi-board cluster problem based on dynamic programming to get the optimal topology of the FPGA clusters. Experimental results show that typical well-known CNNs with our proposed FPGA cluster topology obtains an average throughput 4.33X than single-board solutions and 1.87X than other state-of-the-art multi-board solutions.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {126–134},
numpages = {9},
keywords = {FPGA clusters, convolution neural networks, machine learning acceleration},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{objectrecognition,
author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik\"{a}inen, Matti},
title = {Deep Learning for Generic Object Detection: A Survey},
year = {2020},
issue_date = {Feb 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01247-4},
doi = {10.1007/s11263-019-01247-4},
abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.},
journal = {Int. J. Comput. Vision},
month = feb,
pages = {261–318},
numpages = {58},
keywords = {Object recognition, Convolutional neural networks, Deep learning, Object detection}
}

@INPROCEEDINGS{facenet,
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FaceNet: A unified embedding for face recognition and clustering}, 
  year={2015},
  volume={},
  number={},
  pages={815-823},
  keywords={Face;Face recognition;Training;Accuracy;Artificial neural networks;Standards;Principal component analysis},
  doi={10.1109/CVPR.2015.7298682}}

@inproceedings {cpucnn,
author = {Yizhi Liu and Yao Wang and Ruofei Yu and Mu Li and Vin Sharma and Yida Wang},
title = {Optimizing {CNN} Model Inference on {CPUs}},
booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {1025--1040},
url = {https://www.usenix.org/conference/atc19/presentation/liu-yizhi},
publisher = {USENIX Association},
month = jul
}

@article{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}

@INPROCEEDINGS{eyeriss,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  booktitle={2016 IEEE International Solid-State Circuits Conference (ISSCC)}, 
  title={14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={262-263},
  keywords={Shape;Random access memory;Arrays;Clocks;Logic arrays;Memory management;Bandwidth},
  doi={10.1109/ISSCC.2016.7418007}}

@INPROCEEDINGS{sparsity,
  author={Lee, Minjae and Park, Seongmin and Kim, Hyungmin and Yoon, Minyong and Lee, Janghwan and Choi, Jun Won and Kim, Nam Sung and Kang, Mingu and Choi, Jungwook},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={SPADE: Sparse Pillar-based 3D Object Detection Accelerator for Autonomous Driving}, 
  year={2024},
  volume={},
  number={},
  pages={454-467},
  keywords={Three-dimensional displays;Convolution;Heuristic algorithms;Energy conservation;Object detection;Vectors;Encoding},
  doi={10.1109/HPCA57654.2024.00041}}

@article{CNNWorkloadMainSource,
author = {Venieris, Stylianos I. and Kouris, Alexandros and Bouganis, Christos-Savvas},
title = {Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3186332},
doi = {10.1145/3186332},
abstract = {In the past decade, Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance in various Artificial Intelligence tasks. To accelerate the experimentation and development of CNNs, several software frameworks have been released, primarily targeting power-hungry CPUs and GPUs. In this context, reconfigurable hardware in the form of FPGAs constitutes a potential alternative platform that can be integrated in the existing deep-learning ecosystem to provide a tunable balance between performance, power consumption, and programmability. In this article, a survey of the existing CNN-to-FPGA toolflows is presented, comprising a comparative study of their key characteristics, which include the supported applications, architectural choices, design space exploration methods, and achieved performance. Moreover, major challenges and objectives introduced by the latest trends in CNN algorithmic research are identified and presented. Finally, a uniform evaluation methodology is proposed, aiming at the comprehensive, complete, and in-depth evaluation of CNN-to-FPGA toolflows.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {56},
numpages = {39},
keywords = {deep learning, FPGA toolflows, Convolutional neural networks}
}

@article{ocnnoriginal,
author = {Wang, Peng-Shuai and Liu, Yang and Guo, Yu-Xiao and Sun, Chun-Yu and Tong, Xin},
title = {O-CNN: octree-based convolutional neural networks for 3D shape analysis},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073608},
doi = {10.1145/3072959.3073608},
abstract = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {72},
numpages = {11},
keywords = {shape segmentation, shape retrieval, octree, object classification, convolutional neural network}
}

@techreport{amddatasheet,
  title        = {{UltraScale™ Architecture and Product Data Sheet: Overview (DS890)}},
  author       = {{AMD}},
  institution  = {{AMD}},
  type         = {Product Data Sheet},
  number       = {DS890 (v4.7)},
  year         = {2025},
  month        = may,
  note         = {Revision 4.7, Release Date: May 13, 2025},
  url          = {https://docs.amd.com/v/u/en-US/ds890-ultrascale-overview}
}
